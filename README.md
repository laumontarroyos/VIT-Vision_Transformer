# VIT-Vision_Transformer
O presente trabalho tem por finalidade o experimento did√°tico de realizar o "fine-tuning" de um modelo ViT (Vision Transformer) para classifica√ß√£o de imagens, tarefa t√≠pica do segmento de vis√£o computacional. 

Descrito no artigo de autoria de  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov e outros (2021), publica√ß√£o que pode ser localizada no endere√ßo https://arxiv.org/pdf/2010.11929.pdf, os autores prop√µem uma abordagem para o reconhecimento de imagens usando Transformers, arquitetura que tem alcan√ßado √≥timos resultados em tarefas do processamento de linguagem natural.

O conceito central do artigo √© substituir a abordagem tradicional de processamento de imagens, que trata as imagens como uma grade de pixels, por uma representa√ß√£o textual. Eles dividem a imagem em patches de 16x16 pixels e codificam cada patch em um vetor de palavras (Embedded Patches). Esses vetores de palavras s√£o ent√£o adicionados a vetores posicionais (Posicional embeddings) e alimentados em um modelo Transformer para an√°lise.

Em s√≠ntese, o ViT substitui a entrada de texto por patches da imagem, aplicando as mesmas opera√ß√µes de aten√ß√£o (self-attention layers) e transforma√ß√£o linear, presentes na estrutura tradicional do "Codificador" (Encoder) de um Transformer, de forma a capturar as informa√ß√µes importantes para a constru√ß√£o de uma representa√ß√£o latente, a qual poder√° ser aplicada, em √∫ltima an√°lise, a tarefas espec√≠ficas da vis√£o computacional, como classifica√ß√£o de imagem ou detec√ß√£o de objetos.

O ViT (Vision Transformer) √© treinado em um grande n√∫mero de imagens e mostra resultados promissores em diferentes conjuntos de dados de refer√™ncia, em termos de precis√£o e escalabilidade, em compara√ß√£o com modelos tradicionais de convolu√ß√£o.





![](/img/VIT-Vision-Transformer.jpeg)
figura 1: vis√£o geral do modelo ViT - Vision Transformer, descrito no artigo de autoria de Vaswani e outros (2017).

### Fonte de Dados: Dataset "cats_vs_dogs" hospedado na "Hugging Face"
Como fonte de dados para o "fine-tuning", utilizou-se um subconjunto formado por imagens de gatos e c√£es do "dataset" conhecido como "Asirra" (Animal Species Image Recognition for Restricting Access). Desse "dataset" hospedado na plataforma "Hugging Face" ü§ó (https://huggingface.co/datasets/cats_vs_dogs), utilizou-se 8.427 (oito mil, quatrocentas e vinte e sete) imagens neste trabalho, o que corresponde a cerca de 50% do conte√∫do armazenado.

Sobre o "dataset" Asirra hospedado na plataforma "Kaggle"(https://www.kaggle.com/c/dogs-vs-cats), origem do subconjunto presente na "Hugging Face"ü§ó, segue, abaixo, trecho da descri√ß√£o extra√≠da do pr√≥prio "Kaggle" e mantida na l√≠ngua original em ingl√™s:

The Asirra data set

Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.

Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:

Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. 


# Experimento de "Fine-Tuning"
Ap√≥s fine-tunning do modelo ViT para classifica√ß√£o de imagens a partir do dataset "cats_vs_dogs", observa-se bons resultados de acur√°cia e F1-Score, confome ilustrado no quadro da imagem abaixo. √â necess√°rio lembrar que se fez uso neste trabalho de apenas 50% do dataset disponibilizado na plataforma, de maneira que um treinamento com o dataset completo poder√° apresentar um resultado ainda melhor.

![](/img/modelo-fine-tunning-acuracia.png)


Como esperado, a matriz de confus√£o tamb√©m ilustra os bons resultados alcan√ßados, dada a ordem de grandeza do n√∫mero de acertos na diagonal em cor amarela, relativamente ao de erros na diagonal da outra cor.

![](/img/modelo-fine-tunning-matriz-confusao.png)


# Comparando com modelo Vit disponibilizado na "hugging Face" 
Apenas por curiosidade, resolveu-se comparar os resultados do modelo did√°tico treinado neste trabalho com aquele alcan√ßado por um dos modelos hospedados na plataforma "hugging face" ü§ó  e ajustado para o mesmo dataset "cats_vs_dogs". Entre os modelos existentes atualmente e que podem ser vistos na imagem abaixo, optou-se, aleatoriamente, por usar o segundo mais "baixado", identificado como "nateraw/vit-base-cats-vs-dogs".

![](/img/modelos-hugginface-catsdogs.jpeg)


Escolhido o modelo "nateraw/vit-base-cats-vs-dogs", submeteu-se o mesmo conjunto de testes utilizado para avaliar o nosso modelo did√°tico deste trabalho e alcan√ßou-se os excelentes resultados de acur√°cia e F1-Score abaixo. Trata-se, evidentemente, de uma avalia√ß√£o comparativa apenas superficial. O modelo escolhido na plataforma possui outros ajustes finos n√£o aplicados ao nosso e √© poss√≠vel considerar a possibilidade dele ter sido treinado com um maior conjunto de dados do que o nosso, lembrando que fizemo uso de apenas metade do dataset no presente trabalho e ainda menos no treinamento. Como n√£o organizamos o conjunto de treinamento do modelo da plataforma, tamb√©m √© necess√°rio considerar a possibilidade de nosso conjunto de testes, ou boa parte dele, ter integrado a base utilizada para o treinamento do modelo da "hugging face" ü§ó. Enfim, trata-se de uma compora√ß√£o apenas ilustrativa.

![](/img/modelo-Kaggle-acuracia.png)


Os bons resultados do modelo da "hugging face" ü§ó escolhido tamb√©m est√£o vis√≠veis na matriz de confus√£o abaixo.

![](/img/modelo-Kaggle-matriz-confusao.png)
