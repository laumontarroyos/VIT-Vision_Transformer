# VIT-Vision_Transformer
O presente trabalho tem por finalidade o experimento did√°tico de realizar o "fine-tuning" de um modelo ViT (Vision Transformer) para classifica√ß√£o de imagens, tarefa t√≠pica do segmento de vis√£o computacional. 

Descrito no artigo de autoria de  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov e outros (2021), publica√ß√£o que pode ser localizada no endere√ßo https://arxiv.org/pdf/2010.11929.pdf, os autores prop√µem uma abordagem para o reconhecimento de imagens usando Transformers, arquitetura que tem alcan√ßado √≥timos resultados em tarefas do processamento de linguagem natural.

O conceito central do artigo √© substituir a abordagem tradicional de processamento de imagens, que trata as imagens como uma grade de pixels, por uma representa√ß√£o textual. Eles dividem a imagem em patches de 16x16 pixels e codificam cada patch em um vetor de palavras (Embedded Patches). Esses vetores de palavras s√£o ent√£o adicionados a vetores posicionais (Posicional embeddings) e alimentados em um modelo Transformer para an√°lise.

Em s√≠ntese, o ViT substitui a entrada de texto por patches da imagem, aplicando as mesmas opera√ß√µes de aten√ß√£o (self-attention layers) e transforma√ß√£o linear, presentes na estrutura tradicional do "Codificador" (Encoder) de um Transformer, de forma a capturar as informa√ß√µes importantes para a constru√ß√£o de uma representa√ß√£o latente, a qual poder√° ser aplicada, em √∫ltima an√°lise, a tarefas espec√≠ficas da vis√£o computacional, como classifica√ß√£o de imagem ou detec√ß√£o de objetos.

O ViT (Vision Transformer) √© treinado em um grande n√∫mero de imagens e mostra resultados promissores em diferentes conjuntos de dados de refer√™ncia, em termos de precis√£o e escalabilidade, em compara√ß√£o com modelos tradicionais de convolu√ß√£o.





![](/img/VIT-Vision-Transformer.jpeg)
figura 1: vis√£o geral do modelo ViT - Vision Transformer, descrito no artigo de autoria de Vaswani e outros (2017).

### Fonte de Dados: Dataset "cats_vs_dogs" hospedado na "Hugging Face"
Como fonte de dados para o "fine-tuning", utilizou-se um subconjunto formado por imagens de gatos e c√£es do "dataset" conhecido como "Asirra" (Animal Species Image Recognition for Restricting Access). Desse "dataset" hospedado na plataforma "Hugging Face" ü§ó (https://huggingface.co/datasets/cats_vs_dogs), utilizou-se 8.427 (oito mil, quatrocentas e vinte e sete) imagens neste trabalho, o que corresponde a cerca de 50% do conte√∫do armazenado.

Sobre o "dataset" Asirra hospedado na plataforma "Kaggle"(https://www.kaggle.com/c/dogs-vs-cats), origem do subconjunto presente na "Hugging Face"ü§ó, segue, abaixo, trecho da descri√ß√£o extra√≠da do pr√≥prio "Kaggle" e mantida na l√≠ngua original em ingl√™s:

The Asirra data set

Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.

Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:

Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. 

# Experimento de "Fine-Tune"
![](/img/modelo-fine-tunning-acuracia.png)

![](/img/modelo-fine-tunning-matriz-confusao.png)

# Comparando com modelo Vit disponibilizado na "huggin Face" 
![](/img/modelo-Kaggle-acuracia.png)

![](/img/modelo-Kaggle-matriz-confusao.png)
